# Speech AI Suite - Complete Project Overview

## ğŸ¯ Project Summary

**Speech AI Suite** is a professional, production-ready multi-task speech analysis platform that performs four critical audio classification tasks using state-of-the-art self-supervised learning models. This project combines cutting-edge deep learning with practical web application design, achieving 79.14% accuracy on emotion recognition and serving as a comprehensive reference for speech representation learning.

**Research Paper Foundation:**
- *"From Raw Speech to Fixed Representations: A Comprehensive Evaluation of Speech Embedding Techniques"* (IEEE/ACM 2024)

---

## ğŸ“Š Four Core Tasks

| Task | Model | Classifier | Accuracy | Dataset | Classes |
|------|-------|-----------|----------|---------|---------|
| **Emotion Classification** | HuBERT-large | SVM | 79.14% | CREMA-D | 6 (Neutral, Happy, Sad, Angry, Fear, Disgust) |
| **Gender Identification** | WavLM-base-plus | Logistic Regression | â€” | Mixed | 2 (Male, Female) |
| **Intent Classification** | WavLM-base-plus | SVM | â€” | SLURP | 20 (Voice Commands) |
| **Speaker Identification** | XLSR-53 | Logistic Regression | â€” | Mixed | Variable (Speaker IDs) |

---

## ğŸ—ï¸ System Architecture

```
Speech AI Suite
â”œâ”€â”€ Frontend (Bootstrap 5 + HTML/CSS/JS)
â”‚   â”œâ”€â”€ Emotion Classification Page
â”‚   â”œâ”€â”€ Gender Identification Page
â”‚   â”œâ”€â”€ Intent Classification Page
â”‚   â”œâ”€â”€ Speaker Identification Page
â”‚   â””â”€â”€ About Page (Project Documentation)
â”‚
â”œâ”€â”€ Backend (Flask - Python Web Framework)
â”‚   â”œâ”€â”€ Audio Input Handler
â”‚   â”œâ”€â”€ Inference Services (4 parallel modules)
â”‚   â””â”€â”€ REST API Endpoints
â”‚
â””â”€â”€ ML Pipeline (PyTorch + scikit-learn)
    â”œâ”€â”€ Data Preprocessing
    â”œâ”€â”€ Feature Extraction (Self-supervised Models)
    â”œâ”€â”€ Model Training & Evaluation
    â””â”€â”€ Inference Modules
```

---

## ğŸ”„ Complete Data Processing Pipeline

### Stage 1: Data Preprocessing
**Input:** Raw audio files (WAV, MP3, FLAC, OGG, M4A, WebM)
**Process:**
1. Audio loading and normalization
2. Resampling to 16kHz (standard for speech models)
3. Duration filtering (remove too short/long samples)
4. Label mapping and standardization
5. Data split: Train (70%), Validation (15%), Test (15%)

**Output:** Preprocessed dataset metadata (CSV format)

### Stage 2: Feature Extraction
**Input:** Preprocessed audio files
**Process:**
1. Load pre-trained self-supervised model (HuBERT, WavLM, or XLSR-53)
2. Extract final hidden layer representations
3. Apply pooling strategy (mean, std, or concatenation)
4. Normalize embeddings using StandardScaler

**Mathematical Operation:**
```
embedding = Model(audio) â†’ hidden_states[last_layer]
pooled_embedding = Pool(hidden_states)
normalized_embedding = StandardScaler.transform(pooled_embedding)
```

**Output:** Fixed-dimensional embeddings (1024-2048 dimensions)

### Stage 3: Dimensionality Reduction
**Input:** High-dimensional embeddings
**Process:**
1. Apply PCA to reduce to 200 components
2. Fit scaler on training data
3. Transform validation/test data

**Mathematical Operation:**
```
X_reduced = PCA(n_components=200).fit_transform(X_embeddings)
```

**Output:** 200-dimensional vectors

### Stage 4: Model Training
**Input:** Reduced embeddings + labels
**Process (varies by task):**
- **SVM:** Multi-class SVM with RBF kernel, hyperparameter tuning via GridSearchCV
- **Logistic Regression:** L2 regularization, balanced class weights
- **Cross-validation:** 5-fold CV for robust performance estimation

**Output:** Serialized model + scaler + label encoder (`.pkl` files)

### Stage 5: Inference
**Input:** New audio file (user upload/recording)
**Process:**
1. Load audio with audio processing library
2. Extract embedding using same model as training
3. Scale embedding using training scaler
4. Predict using trained classifier
5. Get probability scores using `predict_proba()`

**Output:** Predicted label + confidence scores (probabilities)

---

## ğŸ”¬ Deep Learning Models Used

### 1. HuBERT-large (Emotion Classification)
- **Full Name:** Hidden-Unit BERT for Self-supervised Speech Representation
- **Publisher:** Meta AI (Facebook)
- **Architecture:** Transformer-based with 24 layers, 1024 hidden dimensions
- **Output Dimension:** 1024-dimensional embeddings
- **Pre-training:** Masked prediction on 960 hours of Libri-Light unlabeled data
- **Why Used:** Best performance for emotion classification in our experiments

### 2. WavLM-base-plus (Gender & Intent)
- **Full Name:** Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing
- **Publisher:** Microsoft Research
- **Architecture:** 12 transformer layers, 768 hidden dimensions
- **Output Dimension:** 768-dimensional embeddings
- **Pre-training:** 10,000 hours of speech with masked prediction
- **Why Used:** Efficient, fast inference while maintaining good accuracy

### 3. XLSR-53 (Speaker Identification)
- **Full Name:** Wav2Vec 2.0 XLSR (Cross-Lingual Speech Representations)
- **Publisher:** Meta AI
- **Architecture:** 12 transformer layers, 1024 hidden dimensions
- **Output Dimension:** 1024-dimensional embeddings
- **Pre-training:** 56,000 hours of multilingual speech
- **Why Used:** Robust speaker identification across languages and accents

---

## ğŸ“ Mathematical Framework

### Feature Extraction Formula

```
E = PoolingStrategy(HiddenStates[-1])

Where:
- E = Output embedding (fixed-dimension vector)
- HiddenStates[-1] = Last transformer layer output (sequence of vectors)
- PoolingStrategy âˆˆ {mean, max, std, concatenation}

Mean Pooling: E = mean(HiddenStates[-1])
Std Pooling: E = std(HiddenStates[-1])
Concatenation: E = concatenate(mean, std) â†’ 2x dimensions
```

### Classification Formula

**For SVM:**
```
y_pred = sign(Î£ Î±_i * K(x, x_i) + b)

Where:
- K = RBF kernel: K(x,x') = exp(-Î³||x-x'||Â²)
- Î³ (gamma) = 1/n_features
- C (regularization) = 1.0
```

**For Logistic Regression:**
```
P(y=class_i) = exp(w_i Â· x + b_i) / Î£ exp(w_j Â· x + b_j)

Where:
- w_i = weights for class i
- b_i = bias for class i
- x = input embedding (scaled)
```

### Cross-Validation Strategy

```
For each of k=5 folds:
  1. Split data into train (80%) and validation (20%)
  2. Train classifier on train fold
  3. Evaluate on validation fold
  4. Record metrics

Final_Accuracy = mean(fold_accuracies)
```

---

## ğŸ’¾ Data Formats & Datasets

### Input Audio Formats Supported
- **WAV:** Uncompressed PCM, 16-bit
- **MP3:** Compressed, variable bitrate
- **FLAC:** Lossless compression
- **OGG/Opus:** Open-source compression
- **M4A:** Apple audio format
- **WebM:** Browser-native recording format

### Datasets Used

#### CREMA-D (Emotion Classification)
- **Size:** ~7,500 utterances
- **Duration:** ~47 hours total
- **Speakers:** 91 unique speakers
- **Emotions:** 6 (Neutral, Happy, Sad, Angry, Fear, Disgust)
- **Language:** English
- **Recording:** Studio quality, multiple takes per emotion
- **Format:** WAV files at 16kHz

#### SLURP (Intent Classification)
- **Size:** ~63,000 utterances
- **Task:** Spoken Language Understanding
- **Intents:** 20+ voice command categories
- **Domains:** Smart home, entertainment, productivity
- **Language:** English
- **Format:** Audio files at 16kHz

---

## ğŸ› ï¸ Technology Stack

### Backend Framework
- **Flask:** Lightweight Python web framework
- **Werkzeug:** WSGI utilities for request handling
- **Python 3.10+:** Runtime environment

### Deep Learning Libraries
- **PyTorch:** Tensor operations, model inference
- **Transformers (HuggingFace):** Pre-trained models loading
- **torchaudio:** Audio processing and resampling
- **librosa:** Audio feature computation
- **soundfile:** Audio file I/O

### Machine Learning & Data Science
- **scikit-learn:** SVM, Logistic Regression, PCA, StandardScaler
- **numpy:** Numerical computations
- **pandas:** Data manipulation (CSV handling)
- **scipy:** Scientific computing utilities

### Frontend
- **Bootstrap 5:** Responsive UI framework
- **HTML5:** Semantic markup
- **CSS3:** Styling with gradients and animations
- **JavaScript (Vanilla):** Audio recording/playback, form handling
- **Bootstrap Icons:** SVG icon library

### Development & Deployment
- **Git:** Version control
- **Git LFS:** Large file storage (for .pkl models)
- **GitHub:** Repository hosting
- **Virtual Environment:** Python dependency isolation

---

## ğŸš€ Project Development Workflow

### 1. Data Preparation Phase
```
Raw Audio Files (CREMA-D, SLURP)
         â†“
   1_data_preprocessing.py
         â†“
Normalized CSV Metadata
```

### 2. Feature Extraction Phase
```
Audio Files + Preprocessing Metadata
         â†“
   2_wavlm_feature_extraction.py
         â†“
NumPy Arrays (.npz files)
Embedding Dimensions: 1024 or 768
```

### 3. Model Training Phase
```
Extracted Embeddings + Labels
         â†“
   3_train_classifiers.py
         â†“
Trained Models (.pkl files)
- SVM/LogReg classifier
- StandardScaler
- LabelEncoder
```

### 4. Evaluation Phase
```
Trained Models + Test Set
         â†“
   4_evaluation_metrics.py
         â†“
Metrics (Accuracy, F1, Precision, Recall)
Confusion Matrices
```

### 5. Deployment Phase
```
Trained Models (.pkl)
         â†“
Backend Flask Server
         â†“
REST API Endpoints
         â†“
Web UI (HTML/CSS/JS)
```

---

## ğŸ“ Project Structure

```
Emotion-classification/
â”œâ”€â”€ backend/                           # Flask web application
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ app.py                    # Main Flask application (11 routes)
â”‚   â”‚   â”œâ”€â”€ templates/                # HTML templates (Jinja2)
â”‚   â”‚   â”‚   â”œâ”€â”€ base.html            # Base template with navbar
â”‚   â”‚   â”‚   â”œâ”€â”€ emotion.html         # Emotion classification page
â”‚   â”‚   â”‚   â”œâ”€â”€ gender.html          # Gender identification page
â”‚   â”‚   â”‚   â”œâ”€â”€ intent.html          # Intent classification page
â”‚   â”‚   â”‚   â”œâ”€â”€ speaker.html         # Speaker identification page
â”‚   â”‚   â”‚   â”œâ”€â”€ about.html           # Project documentation
â”‚   â”‚   â”‚   â””â”€â”€ index.html           # Homepage
â”‚   â”‚   â””â”€â”€ static/
â”‚   â”‚       â”œâ”€â”€ css/
â”‚   â”‚       â”‚   â”œâ”€â”€ styles.css       # Main stylesheet (550+ lines)
â”‚   â”‚       â”‚   â””â”€â”€ custom.css
â”‚   â”‚       â”œâ”€â”€ js/                  # JavaScript files
â”‚   â”‚       â””â”€â”€ images/              # Logo, icons
â”‚   â”œâ”€â”€ services/                     # Inference services
â”‚   â”‚   â”œâ”€â”€ emotion.py               # Emotion classification (126 lines)
â”‚   â”‚   â”œâ”€â”€ gender.py                # Gender identification
â”‚   â”‚   â”œâ”€â”€ intent.py                # Intent classification
â”‚   â”‚   â”œâ”€â”€ speaker.py               # Speaker identification
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â””â”€â”€ audio.py             # Audio loading & feature extraction
â”‚   â””â”€â”€ config.py                     # Configuration (paths, models)
â”‚
â”œâ”€â”€ ml_models/                         # ML training & artifacts
â”‚   â”œâ”€â”€ models/                        # Pre-trained models (15 .pkl files, 600+ MB)
â”‚   â”‚   â”œâ”€â”€ emotion_model_svm.pkl     # SVM classifier (143.78 MB)
â”‚   â”‚   â”œâ”€â”€ emotion_scaler.pkl        # StandardScaler
â”‚   â”‚   â”œâ”€â”€ emotion_label_encoder.pkl # LabelEncoder
â”‚   â”‚   â”œâ”€â”€ gender_classifier.pkl     # Logistic Regression
â”‚   â”‚   â”œâ”€â”€ intent_classifier.pkl     # SVM classifier (16.28 MB)
â”‚   â”‚   â””â”€â”€ [More model files...]
â”‚   â”œâ”€â”€ src/                          # Training scripts
â”‚   â”‚   â”œâ”€â”€ 1_data_preprocessing.py   # Data loading & cleaning
â”‚   â”‚   â”œâ”€â”€ 2_wavlm_feature_extraction.py  # Embedding extraction
â”‚   â”‚   â”œâ”€â”€ 3_train_classifiers.py    # Model training
â”‚   â”‚   â”œâ”€â”€ 4_evaluation_metrics.py   # Model evaluation
â”‚   â”‚   â””â”€â”€ 5_visualization_umap.py   # t-SNE/UMAP visualization
â”‚   â”œâ”€â”€ data/                         # Raw datasets
â”‚   â”‚   â”œâ”€â”€ CREMA-D/                 # Emotion dataset
â”‚   â”‚   â”œâ”€â”€ IEMOCAP/                 # Alternative emotion dataset
â”‚   â”‚   â””â”€â”€ processed/                # CSV metadata
â”‚   â”œâ”€â”€ results/                      # Training results
â”‚   â”‚   â”œâ”€â”€ confusion_matrix_*.csv    # Classification matrices
â”‚   â”‚   â”œâ”€â”€ evaluation_results_*.json # Metrics
â”‚   â”‚   â””â”€â”€ test_predictions.csv      # Predictions on test set
â”‚   â””â”€â”€ scripts/                      # Utility scripts
â”‚       â”œâ”€â”€ train_gender_model.py
â”‚       â”œâ”€â”€ train_intent_model.py
â”‚       â”œâ”€â”€ verify_setup.py
â”‚       â””â”€â”€ download_samples.py
â”‚
â”œâ”€â”€ docs/                              # Documentation
â”‚   â”œâ”€â”€ PROJECT_OVERVIEW.md           # This file (complete overview)
â”‚   â”œâ”€â”€ EMOTION_CLASSIFICATION.md     # Emotion task details
â”‚   â”œâ”€â”€ GENDER_IDENTIFICATION.md      # Gender task details
â”‚   â”œâ”€â”€ INTENT_CLASSIFICATION.md      # Intent task details
â”‚   â”œâ”€â”€ SPEAKER_IDENTIFICATION.md     # Speaker task details
â”‚   â”œâ”€â”€ ARCHITECTURE.md               # System architecture
â”‚   â”œâ”€â”€ SETUP_GUIDE.md                # Installation guide
â”‚   â””â”€â”€ QUICKSTART.md                 # Quick start guide
â”‚
â”œâ”€â”€ .gitattributes                     # Git LFS configuration
â”œâ”€â”€ requirements.txt                   # Python dependencies (40+ packages)
â”œâ”€â”€ README.md                          # Quick project description
â””â”€â”€ Emotion-classification.code-workspace  # VS Code workspace
```

---

## ğŸ‘¥ Development Team

| Name | Role | Expertise | Contribution |
|------|------|-----------|--------------|
| **Sk Inthiyaz** | ML Engineer & Project Lead | Architecture, Integration | Project design, backend/frontend integration, system architecture |
| **Romith Singh** | ML Engineer & Team Lead | Speaker Recognition | Speaker identification model, optimization techniques |
| **Rohin Kumar** | ML Engineer | Gender Classification | Gender identification, feature engineering |
| **Sahasra Ganji** | ML Engineer & Data Engineer | Intent Recognition | Intent classification, dataset preparation, data engineering |
| **Rashmitha** | ML Engineer & Research Engineer | Research, Bug Fixing | Bug resolution, documentation, research support |

---

## ğŸ“ Key Concepts Explained

### Self-Supervised Learning
- Models are pre-trained on **massive unlabeled speech data** (10,000-56,000 hours)
- Learn general speech representations without manual annotation
- Fine-tuning layers frozen; use embeddings as features
- Dramatically reduces annotation cost for downstream tasks

### Transfer Learning
- Pre-trained models â†’ Extract embeddings
- Embeddings fed to simple classifiers (SVM, Logistic Regression)
- Avoids training deep models from scratch
- Leverages knowledge from large-scale pre-training

### Dimensionality Reduction (PCA)
- Embeddings are 768-1024 dimensional (very high)
- PCA reduces to 200 dimensions
- Removes noise while preserving variance
- Reduces memory footprint and inference time

### Cross-Validation
- Protects against overfitting
- Uses 5 folds for robust performance estimation
- Reports mean accuracy across all folds
- 79.14% = average across 5 folds for emotion

---

## âœ¨ Key Features

1. **Real-time Audio Processing**
   - Record directly from browser microphone
   - Upload pre-recorded files
   - Support for 6+ audio formats

2. **Multi-Model Support**
   - 4 different self-supervised models
   - Mix-and-match for different tasks
   - Easy to add new models

3. **High Accuracy**
   - Emotion: 79.14% on CREMA-D
   - Leverages state-of-the-art SSL models
   - 5-fold cross-validation for reliability

4. **Production Ready**
   - Proper error handling and logging
   - Git LFS for large model files
   - Modular architecture for maintenance
   - Comprehensive documentation

5. **Optimized for CPU**
   - No GPU requirement
   - Fast inference (2-5 seconds per audio)
   - Suitable for deployment on modest hardware

6. **Beautiful UI**
   - Responsive design (works on mobile/tablet)
   - Gradient backgrounds with animations
   - Real-time audio visualization
   - Professional styling

---

## ğŸ” Model Comparison

### Why Different Models for Different Tasks?

| Task | Model | Reason |
|------|-------|--------|
| **Emotion** | HuBERT-large | Large model needed for fine-grained emotion distinction (6 classes) |
| **Gender** | WavLM-base-plus | Simple binary classification; faster inference sufficient |
| **Intent** | WavLM-base-plus | Medium complexity (20 classes); good speed/accuracy tradeoff |
| **Speaker** | XLSR-53 | Multilingual robustness; handles diverse speaker accents |

---

## ğŸ¯ Interview Confidence Points

When presenting this project in interviews, emphasize:

1. **Research Foundation**
   - Grounded in IEEE/ACM 2024 research paper
   - Evaluated 3 SOTA self-supervised models
   - Published methodology

2. **End-to-End Pipeline**
   - Data preprocessing â†’ Feature extraction â†’ Training â†’ Inference
   - Each stage documented with mathematical formulas
   - Reproducible with provided scripts

3. **Production Considerations**
   - Git LFS for managing large model files
   - Error handling for unsupported formats
   - Configurable batch processing for CPU
   - Comprehensive logging and monitoring

4. **Architecture Decisions**
   - Why each model was chosen
   - Trade-offs between accuracy and speed
   - Dimensionality reduction rationale
   - Cross-validation strategy

5. **Reproducibility**
   - All hyperparameters documented
   - Training scripts with fixed random seeds
   - Version control of all code and data

6. **Scalability**
   - Can add new tasks (follow 4-stage pipeline)
   - Modular service design
   - Easy to deploy on cloud (AWS, Azure, GCP)

---

## ğŸ“š Recommended Reading Order

1. **Start Here:** This file (PROJECT_OVERVIEW.md)
2. **Then:** EMOTION_CLASSIFICATION.md (most developed task)
3. **Next:** GENDER_IDENTIFICATION.md, INTENT_CLASSIFICATION.md, SPEAKER_IDENTIFICATION.md
4. **Finally:** ARCHITECTURE.md (system design details)

---

## ğŸš€ Getting Started

### Installation
```bash
# Clone repository
git clone https://github.com/sk-inthiyaz/Emotion-classification.git
cd Emotion-classification

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Download models (via Git LFS)
git lfs pull

# Run Flask app
cd backend/app
python app.py
```

### Access the Application
- **URL:** http://localhost:5000
- **Emotion Classification:** http://localhost:5000/emotion
- **Gender Identification:** http://localhost:5000/gender
- **Intent Classification:** http://localhost:5000/intent
- **Speaker Identification:** http://localhost:5000/speaker
- **About/Documentation:** http://localhost:5000/about

---

## ğŸ“Š Performance Summary

| Metric | Value |
|--------|-------|
| **Emotion Accuracy** | 79.14% (5-fold CV on CREMA-D) |
| **Emotion F1-Score** | 0.78 (macro average) |
| **Inference Time** | 2-5 seconds (CPU, depending on audio length) |
| **Model Download Size** | ~600 MB (all .pkl files via Git LFS) |
| **Memory Usage** | ~2 GB (during feature extraction) |
| **Supported Languages** | English (main), multilingual support via XLSR-53 |

---

## ğŸ“ License & Attribution

- **Research Paper:** IEEE/ACM 2024
- **Models:** HuggingFace (open-source pre-trained models)
- **Datasets:** CREMA-D, SLURP (academic use)
- **Code:** Open-source, GitHub repository

---

## ğŸ”— Related Links

- **HuBERT Paper:** https://arxiv.org/abs/2106.07447
- **WavLM Paper:** https://arxiv.org/abs/2110.13900
- **XLSR Paper:** https://arxiv.org/abs/2006.13979
- **CREMA-D Dataset:** https://github.com/CheyneyComputerScience/CREMA-D
- **SLURP Dataset:** https://github.com/parietal-io/slurp
- **HuggingFace Models:** https://huggingface.co

---

## âœ… Project Completion Status

- âœ… Data preprocessing pipeline
- âœ… Feature extraction (3 models)
- âœ… Model training & evaluation
- âœ… Inference service (4 tasks)
- âœ… Web UI with Bootstrap
- âœ… Documentation & README
- âœ… Git LFS for large files
- âœ… Production-ready code

**Created:** December 2024
**Last Updated:** December 11, 2025
**Repository:** https://github.com/sk-inthiyaz/Emotion-classification

---

*This documentation serves as a comprehensive reference for understanding, implementing, and extending the Speech AI Suite project.*
